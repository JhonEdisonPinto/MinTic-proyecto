{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78280e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mintic_project.data_loader import procesar_siniestros\n",
    "from mintic_project.feature_engineering import DatasetPredictor\n",
    "\n",
    "print('✓ Librerías cargadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447dae34",
   "metadata": {},
   "source": [
    "## 1. Cargar y procesar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d058dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar datos (pequeño dataset para pruebas)\n",
    "df1, df2 = procesar_siniestros(directorio_salida='data', limite_registros=5000)\n",
    "print(f'Datasets descargados: {df1.shape} y {df2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082b105",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering para predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356da4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para predicción\n",
    "predictor = DatasetPredictor()\n",
    "df_prediccion = predictor.preparar_dataset_completo(df1, df2)\n",
    "\n",
    "print(f'Dataset procesado: {df_prediccion.shape}')\n",
    "print(f'\\nColumnas generadas: {list(df_prediccion.columns[:20])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b706cd",
   "metadata": {},
   "source": [
    "## 3. Generar contexto para RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c265cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar contexto para RAG (Retrieval-Augmented Generation)\n",
    "contexto_rag = predictor.generar_contexto_rag(df_prediccion)\n",
    "\n",
    "print('Contextos generados para RAG:')\n",
    "for tema, contexto in contexto_rag.items():\n",
    "    print(f'\\n--- {tema.upper()} ---')\n",
    "    print(contexto[:200] + '...' if len(contexto) > 200 else contexto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d539532",
   "metadata": {},
   "source": [
    "## 4. Ejemplo: Predicción simple con Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preparar datos para modelo\n",
    "# Usar 'gravedad' como target (si existe)\n",
    "if 'gravedad' in df_prediccion.columns:\n",
    "    # Filtrar columnas numéricas y encoded\n",
    "    feature_cols = [col for col in df_prediccion.columns \n",
    "                   if 'encoded' in col or 'normalized' in col or col in ['edad', 'hora', 'mes', 'distancia_centro']]\n",
    "    \n",
    "    # Preparar X y y\n",
    "    X = df_prediccion[feature_cols].fillna(0)\n",
    "    y = df_prediccion['gravedad'].fillna('DESCONOCIDA')\n",
    "    \n",
    "    print(f'Features: {len(feature_cols)}')\n",
    "    print(f'Clases objetivo: {y.unique()}')\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print('\\nEntrenando modelo Random Forest...')\n",
    "    modelo = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    modelo.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluar\n",
    "    score = modelo.score(X_test, y_test)\n",
    "    print(f'✓ Accuracy: {score:.3f}')\n",
    "    \n",
    "    # Importancia de features\n",
    "    importancia = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importancia': modelo.feature_importances_\n",
    "    }).sort_values('importancia', ascending=False)\n",
    "    \n",
    "    print(f'\\nTop 10 features más importantes:')\n",
    "    print(importancia.head(10).to_string())\n",
    "else:\n",
    "    print('Columna \"gravedad\" no disponible')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5259cb",
   "metadata": {},
   "source": [
    "## 5. Integración con LangChain (ejemplo básico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cómo usar el contexto con LangChain\n",
    "# Nota: Requiere OPENAI_API_KEY configurada en .env\n",
    "\n",
    "ejemplo_prompt = f\"\"\"\n",
    "Contexto de datos de siniestros viales:\n",
    "{contexto_rag.get('general', 'N/A')}\n",
    "\n",
    "Distribución por jornada:\n",
    "{contexto_rag.get('jornada', 'N/A')}\n",
    "\n",
    "Pregunta: ¿En qué jornada ocurren más siniestros?\n",
    "Basándote en los datos, proporciona un análisis.\n",
    "\"\"\"\n",
    "\n",
    "print('Ejemplo de prompt para LangChain:')\n",
    "print(ejemplo_prompt)\n",
    "\n",
    "# Para usar con OpenAI:\n",
    "# from langchain.llms import OpenAI\n",
    "# llm = OpenAI(temperature=0.7)\n",
    "# respuesta = llm(ejemplo_prompt)\n",
    "# print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdcd7af",
   "metadata": {},
   "source": [
    "## 6. Guardar modelo y datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guardar dataset procesado\n",
    "df_prediccion.to_parquet('data/siniestros_procesados.parquet', index=False)\n",
    "print('✓ Dataset procesado guardado: data/siniestros_procesados.parquet')\n",
    "\n",
    "# Guardar modelo (si se entrenó)\n",
    "if 'modelo' in locals():\n",
    "    with open('models/modelo_prediccion.pkl', 'wb') as f:\n",
    "        pickle.dump(modelo, f)\n",
    "    print('✓ Modelo guardado: models/modelo_prediccion.pkl')\n",
    "\n",
    "# Guardar contexto RAG\n",
    "import json\n",
    "with open('data/contexto_rag.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(contexto_rag, f, indent=2, ensure_ascii=False)\n",
    "print('✓ Contexto RAG guardado: data/contexto_rag.json')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
